{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca306bb3",
   "metadata": {},
   "source": [
    "\n",
    "# Retail Analytics (pandas vs polars vs PySpark)\n",
    "\n",
    "Este notebook usa o dataset **RetailStoreProductSalesDataset.csv** (15.000 linhas) para mostrar **equivalências práticas** entre:\n",
    "\n",
    "- **pandas** (DataFrame em memória)\n",
    "- **polars** (DataFrame/expressões colunar, bem rápido)\n",
    "- **PySpark** (DataFrame distribuído)\n",
    "\n",
    "Em cada seção você verá **a mesma análise feita 3 vezes** (uma por biblioteca), usando operações comuns do dia a dia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983fdf8",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup e leitura do dataset\n",
    "\n",
    "> Ajuste o caminho se necessário. Aqui estou usando o arquivo montado no ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe58816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Polars\n",
    "import polars as pl\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Caminho do dataset\n",
    "CSV_PATH = r\"/mnt/data/RetailStoreProductSalesDataset.csv\"\n",
    "\n",
    "# Spark session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"retail-analytics-pandas-polars-spark\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Leitura: pandas\n",
    "df_pd = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Leitura: polars\n",
    "df_pl = pl.read_csv(CSV_PATH)\n",
    "\n",
    "# Leitura: spark\n",
    "df_sp = (spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(CSV_PATH))\n",
    "\n",
    "print(\"pandas shape:\", df_pd.shape)\n",
    "print(\"polars shape:\", df_pl.shape)\n",
    "print(\"spark count:\", df_sp.count(), \" | columns:\", len(df_sp.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69689c1",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Visão rápida e checagem de tipos\n",
    "\n",
    "Só pra garantir que estamos olhando a mesma coisa nas 3 libs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pandas\n",
    "display(df_pd.head(3))\n",
    "print(df_pd.dtypes)\n",
    "\n",
    "# polars\n",
    "print(df_pl.head(3))\n",
    "print(df_pl.schema)\n",
    "\n",
    "# spark\n",
    "df_sp.show(3, truncate=False)\n",
    "df_sp.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7a7f4",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Somar uma coluna (ex.: total de `ad_spend`)\n",
    "\n",
    "Pergunta: quanto foi investido em anúncios no período?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2912cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pandas\n",
    "total_ad_pd = df_pd[\"ad_spend\"].sum()\n",
    "\n",
    "# polars\n",
    "total_ad_pl = df_pl[\"ad_spend\"].sum()\n",
    "\n",
    "# spark\n",
    "total_ad_sp = df_sp.agg(F.sum(\"ad_spend\").alias(\"total_ad_spend\")).collect()[0][\"total_ad_spend\"]\n",
    "\n",
    "print(\"Total ad_spend | pandas:\", total_ad_pd)\n",
    "print(\"Total ad_spend | polars:\", total_ad_pl)\n",
    "print(\"Total ad_spend | spark :\", total_ad_sp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4b561",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Filtrar linhas (ex.: dias com desconto alto e estoque baixo)\n",
    "\n",
    "Filtro: `discount > 0.30` **e** `stock_level < 20`  \n",
    "Isso pode indicar risco de ruptura + promoção agressiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pandas\n",
    "high_disc_low_stock_pd = df_pd[(df_pd[\"discount\"] > 0.30) & (df_pd[\"stock_level\"] < 20)]\n",
    "\n",
    "# polars\n",
    "high_disc_low_stock_pl = df_pl.filter((pl.col(\"discount\") > 0.30) & (pl.col(\"stock_level\") < 20))\n",
    "\n",
    "# spark\n",
    "high_disc_low_stock_sp = df_sp.filter((F.col(\"discount\") > 0.30) & (F.col(\"stock_level\") < 20))\n",
    "\n",
    "print(\"Linhas filtradas | pandas:\", len(high_disc_low_stock_pd))\n",
    "print(\"Linhas filtradas | polars:\", high_disc_low_stock_pl.height)\n",
    "print(\"Linhas filtradas | spark :\", high_disc_low_stock_sp.count())\n",
    "\n",
    "display(high_disc_low_stock_pd.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38586129",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Contar uma coluna (não nulos)\n",
    "\n",
    "Vamos contar registros válidos em `customer_sentiment` (deve ser quase tudo, mas é um check rápido).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pandas\n",
    "count_sent_pd = df_pd[\"customer_sentiment\"].count()\n",
    "\n",
    "# polars\n",
    "count_sent_pl = df_pl[\"customer_sentiment\"].count()\n",
    "\n",
    "# spark\n",
    "count_sent_sp = df_sp.select(F.count(\"customer_sentiment\").alias(\"cnt\")).collect()[0][\"cnt\"]\n",
    "\n",
    "print(\"Count customer_sentiment | pandas:\", count_sent_pd)\n",
    "print(\"Count customer_sentiment | polars:\", count_sent_pl)\n",
    "print(\"Count customer_sentiment | spark :\", count_sent_sp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787973f4",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Contar valores distintos (ex.: `weather_index` arredondado)\n",
    "\n",
    "Como é numérico contínuo, contar distintos “cru” costuma ser gigante.  \n",
    "Então vamos arredondar (`weather_index` com 1 casa) e contar distintos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pandas\n",
    "weather_rounded_pd = df_pd[\"weather_index\"].round(1)\n",
    "nunique_weather_pd = weather_rounded_pd.nunique()\n",
    "\n",
    "# polars\n",
    "nunique_weather_pl = (df_pl\n",
    "    .select(pl.col(\"weather_index\").round(1).n_unique().alias(\"n_unique_weather_round1\"))\n",
    "    .item())\n",
    "\n",
    "# spark\n",
    "nunique_weather_sp = (df_sp\n",
    "    .select(F.round(\"weather_index\", 1).alias(\"weather_round1\"))\n",
    "    .agg(F.countDistinct(\"weather_round1\").alias(\"n_unique_weather_round1\"))\n",
    "    .collect()[0][\"n_unique_weather_round1\"])\n",
    "\n",
    "print(\"Distinct weather_index (round 1) | pandas:\", nunique_weather_pd)\n",
    "print(\"Distinct weather_index (round 1) | polars:\", nunique_weather_pl)\n",
    "print(\"Distinct weather_index (round 1) | spark :\", nunique_weather_sp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97da3f3",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Agrupar e somar (group by)\n",
    "\n",
    "Vamos criar um *bucket* simples de sentimento e somar `footfall` por faixa.  \n",
    "Isso dá uma ideia de “tráfego total” associado a diferentes níveis de sentimento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função de bucketing (para manter a lógica simples e idêntica)\n",
    "def sentiment_bucket(x):\n",
    "    if x < -0.3:\n",
    "        return \"negativo\"\n",
    "    elif x > 0.3:\n",
    "        return \"positivo\"\n",
    "    return \"neutro\"\n",
    "\n",
    "# pandas\n",
    "df_pd_tmp = df_pd.copy()\n",
    "df_pd_tmp[\"sent_bucket\"] = df_pd_tmp[\"customer_sentiment\"].apply(sentiment_bucket)\n",
    "gb_pd = (df_pd_tmp.groupby(\"sent_bucket\", as_index=False)[\"footfall\"].sum()\n",
    "         .sort_values(\"footfall\", ascending=False))\n",
    "\n",
    "# polars\n",
    "df_pl_tmp = df_pl.with_columns(\n",
    "    pl.when(pl.col(\"customer_sentiment\") < -0.3).then(pl.lit(\"negativo\"))\n",
    "     .when(pl.col(\"customer_sentiment\") > 0.3).then(pl.lit(\"positivo\"))\n",
    "     .otherwise(pl.lit(\"neutro\"))\n",
    "     .alias(\"sent_bucket\")\n",
    ")\n",
    "gb_pl = (df_pl_tmp\n",
    "         .group_by(\"sent_bucket\")\n",
    "         .agg(pl.col(\"footfall\").sum().alias(\"footfall_sum\"))\n",
    "         .sort(\"footfall_sum\", descending=True))\n",
    "\n",
    "# spark\n",
    "df_sp_tmp = (df_sp\n",
    "    .withColumn(\n",
    "        \"sent_bucket\",\n",
    "        F.when(F.col(\"customer_sentiment\") < -0.3, F.lit(\"negativo\"))\n",
    "         .when(F.col(\"customer_sentiment\") > 0.3, F.lit(\"positivo\"))\n",
    "         .otherwise(F.lit(\"neutro\"))\n",
    "    )\n",
    ")\n",
    "gb_sp = (df_sp_tmp\n",
    "         .groupBy(\"sent_bucket\")\n",
    "         .agg(F.sum(\"footfall\").alias(\"footfall_sum\"))\n",
    "         .orderBy(F.col(\"footfall_sum\").desc()))\n",
    "\n",
    "display(gb_pd)\n",
    "print(gb_pl)\n",
    "gb_sp.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c057a9",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Transformar colunas em linhas (melt / unpivot)\n",
    "\n",
    "Exemplo: pegar um subconjunto de features e transformar para formato “long”  \n",
    "para facilitar plots, comparações e checagem de distribuição.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c88e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES = [\"price\", \"discount\", \"promotion_intensity\", \"footfall\", \"ad_spend\"]\n",
    "\n",
    "# Para não explodir tamanho, vamos usar uma amostra\n",
    "sample_pd = df_pd[FEATURES].sample(1000, random_state=42)\n",
    "\n",
    "# pandas: melt\n",
    "long_pd = sample_pd.melt(var_name=\"feature\", value_name=\"value\")\n",
    "\n",
    "# polars: unpivot (o mesmo efeito)\n",
    "sample_pl = df_pl.select(FEATURES).sample(n=1000, seed=42)\n",
    "long_pl = sample_pl.unpivot(on=FEATURES, variable_name=\"feature\", value_name=\"value\")\n",
    "\n",
    "# spark: stack (unpivot)\n",
    "sample_sp = df_sp.select(*FEATURES).sample(False, 1000/df_pd.shape[0], seed=42)\n",
    "expr = \"stack({n}, {pairs}) as (feature, value)\".format(\n",
    "    n=len(FEATURES),\n",
    "    pairs=\", \".join([f\"'{c}', {c}\" for c in FEATURES])\n",
    ")\n",
    "long_sp = sample_sp.selectExpr(expr)\n",
    "\n",
    "print(\"pandas long shape:\", long_pd.shape)\n",
    "print(\"polars long shape:\", long_pl.shape)\n",
    "print(\"spark long count:\", long_sp.count())\n",
    "\n",
    "display(long_pd.head(10))\n",
    "print(long_pl.head(10))\n",
    "long_sp.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5c747",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Estatísticas descritivas\n",
    "\n",
    "Um “raio-x” rápido das colunas numéricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pandas\n",
    "desc_pd = df_pd.describe()\n",
    "display(desc_pd)\n",
    "\n",
    "# polars\n",
    "desc_pl = df_pl.describe()\n",
    "print(desc_pl)\n",
    "\n",
    "# spark\n",
    "df_sp.describe().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1532a61",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Criar nova coluna (ex.: `effective_price`)\n",
    "\n",
    "Vamos criar `effective_price = price * (1 - discount)`  \n",
    "Isso é uma proxy simples de preço efetivo após desconto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fbdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pandas\n",
    "df_pd_eff = df_pd.copy()\n",
    "df_pd_eff[\"effective_price\"] = df_pd_eff[\"price\"] * (1 - df_pd_eff[\"discount\"])\n",
    "\n",
    "# polars\n",
    "df_pl_eff = df_pl.with_columns(\n",
    "    (pl.col(\"price\") * (1 - pl.col(\"discount\"))).alias(\"effective_price\")\n",
    ")\n",
    "\n",
    "# spark\n",
    "df_sp_eff = df_sp.withColumn(\n",
    "    \"effective_price\",\n",
    "    F.col(\"price\") * (F.lit(1) - F.col(\"discount\"))\n",
    ")\n",
    "\n",
    "print(\"pandas effective_price head:\")\n",
    "display(df_pd_eff[[\"price\",\"discount\",\"effective_price\"]].head(5))\n",
    "\n",
    "print(\"polars effective_price head:\")\n",
    "print(df_pl_eff.select([\"price\",\"discount\",\"effective_price\"]).head(5))\n",
    "\n",
    "print(\"spark effective_price head:\")\n",
    "df_sp_eff.select(\"price\",\"discount\",\"effective_price\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c047207",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Excluir colunas (drop)\n",
    "\n",
    "Suponha que você quer remover colunas para um modelo específico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_drop = [\"competitor_price\", \"return_rate\"]\n",
    "\n",
    "# pandas\n",
    "df_pd_drop = df_pd.drop(columns=cols_to_drop)\n",
    "\n",
    "# polars\n",
    "df_pl_drop = df_pl.drop(cols_to_drop)\n",
    "\n",
    "# spark\n",
    "df_sp_drop = df_sp.drop(*cols_to_drop)\n",
    "\n",
    "print(\"Cols originais:\", len(df_pd.columns))\n",
    "print(\"Depois do drop | pandas:\", len(df_pd_drop.columns))\n",
    "print(\"Depois do drop | polars:\", len(df_pl_drop.columns))\n",
    "print(\"Depois do drop | spark :\", len(df_sp_drop.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3e642",
   "metadata": {},
   "source": [
    "\n",
    "## 11) (Bônus) Ordenar (sort) e pegar Top N\n",
    "\n",
    "Exemplo: Top 10 dias com maior `footfall` (tráfego).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pandas\n",
    "top_pd = df_pd.sort_values(\"footfall\", ascending=False).head(10)\n",
    "\n",
    "# polars\n",
    "top_pl = df_pl.sort(\"footfall\", descending=True).head(10)\n",
    "\n",
    "# spark\n",
    "top_sp = df_sp.orderBy(F.col(\"footfall\").desc()).limit(10)\n",
    "\n",
    "display(top_pd)\n",
    "print(top_pl)\n",
    "top_sp.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe897e5",
   "metadata": {},
   "source": [
    "\n",
    "## Fechamento\n",
    "\n",
    "Você acabou de ver o mesmo conjunto de operações (as mais comuns de análise/engenharia de dados) em **pandas**, **polars** e **PySpark**.\n",
    "\n",
    "Dica de cérebro:  \n",
    "- pandas = “faço agora na memória”  \n",
    "- polars = “faço com expressões colunar rápidas”  \n",
    "- spark = “faço em cluster, com plano de execução”\n",
    "\n",
    "Se quiser, dá pra estender isso com:\n",
    "- joins, window functions, pivot, tratamento de nulos, encoding/categorias\n",
    "- baseline de modelagem (split, treino, métricas) com features derivadas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "title": "Retail Analytics Cheat-Sheet: pandas vs polars vs PySpark"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
